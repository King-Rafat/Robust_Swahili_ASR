{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df59c29b-829f-48ed-8ed4-e005ebefbcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "import librosa\n",
    "import contextlib\n",
    "from mutagen.mp3 import MP3\n",
    "from datasets import load_dataset, Value, Features, Audio, Dataset, concatenate_datasets, DatasetDict\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from transformers import set_seed\n",
    "torch.cuda.empty_cache()\n",
    "set_seed(42)\n",
    "import re\n",
    "# Details: https://huggingface.co/docs/diffusers/optimization/fp16#enable-cudnn-autotuner\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "from transformers import pipeline\n",
    "os.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7570190b-aed6-4879-ac8a-5c2f432a4d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import WhisperForConditionalGeneration, WhisperProcessor\n",
    "import torch\n",
    "from evaluate import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ccd2d1a-2bd5-4b19-b517-c23b9a855d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperForConditionalGeneration, WhisperProcessor\n",
    "from transformers import pipeline\n",
    "from transformers.utils import is_flash_attn_2_available\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-large-v2\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"RafatK/Swahili-Whisper-Largev2-Decodis_FT\", torch_dtype=torch.float16).to(\"cuda\")\n",
    "model.generation_config.input_ids = model.generation_config.forced_decoder_ids\n",
    "model.generation_config.forced_decoder_ids = None\n",
    "forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"swahili\", task=\"transcribe\")\n",
    "\n",
    "pipe = pipeline(\n",
    "  \"automatic-speech-recognition\",\n",
    "  model=model,\n",
    "    processor = \"openai/whisper-large-v2\",\n",
    "    tokenizer = \"openai/whisper-large-v2\",\n",
    "    feature_extractor = \"openai/whisper-large-v2\",\n",
    "  chunk_length_s=15,\n",
    "  device=device,\n",
    "  model_kwargs={\"attn_implementation\": \"flash_attention_2\"} if is_flash_attn_2_available() else {\"attn_implementation\": \"sdpa\"},\n",
    "  generate_kwargs = {\n",
    "                       'num_beams':5,\n",
    "                       'max_new_tokens':440,\n",
    "                       'early_stopping':True,\n",
    "                       }\n",
    ")\n",
    "text_output = pipe(\"audio.wav\")['text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "906ec073-3127-4b4d-b33a-b7883a9ecd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "features = Features(\n",
    "    {   # \"file_name\": Value('string'),\n",
    "        \"path\": Audio(sampling_rate=16000),\n",
    "        \"transcription\": Value(\"string\"),\n",
    "       \n",
    "    }\n",
    ")\n",
    "in_house = datasets.load_dataset(\"csv\", data_files={\"test\": \"openslr/pi_test.csv\"}, delimiter=\"\\t\", features = features)[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95c33f2a-5762-4a67-8560-431caf8ff5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "# fleurs = load_dataset(\"google/fleurs\", \"sw_ke\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ab6807b-d0d6-43cf-b12e-50a7c2be53de",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-large-v2\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"whisper_swhaili\", torch_dtype=torch.float16).to(\"cuda\")\n",
    "forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"swahili\", task=\"transcribe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ac77456-fee5-4bf4-9aa4-9047848b6127",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers.utils import is_flash_attn_2_available\n",
    "from transformers import pipeline\n",
    "from datasets import load_dataset\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "pipe = pipeline(\n",
    "  \"automatic-speech-recognition\",\n",
    "  model=model,\n",
    "    processor = \"openai/whisper-large-v2\",\n",
    "    tokenizer = \"openai/whisper-large-v2\",\n",
    "    feature_extractor = \"openai/whisper-large-v2\",\n",
    "  chunk_length_s=15,\n",
    "  device=device,\n",
    "    model_kwargs={\"attn_implementation\": \"flash_attention_2\"} if is_flash_attn_2_available() else {\"attn_implementation\": \"sdpa\"},\n",
    "    generate_kwargs = {\n",
    "\n",
    "                       'repetition_penalty':1.8,\n",
    "                       'num_beams':5,\n",
    "                       'max_new_tokens':444,\n",
    "                       'early_stopping':True,\n",
    "\n",
    "                       }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22b8b649-bc70-45b9-b7cb-eb58d7160d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fleur_path_test = []\n",
    "fleur_trans_test = []\n",
    "for i in range(len(fleurs)):\n",
    "    s = fleurs[i]['path']\n",
    "    t = fleurs[i]['audio']['path']\n",
    "    path  = \"\\\\\".join(s.split(\"\\\\\")[:-1])+\"\\\\\"+t\n",
    "    fleur_path_test.append(path)\n",
    "    fleur_trans_test.append(fleurs[i][\"raw_transcription\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c59e922-3993-4b32-96f8-3b240abffc23",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred = []\n",
    "for i in range(len(fleur_path_test)):\n",
    "    print(i)\n",
    "    prediction = pipe(fleur_path_test[i]['path']['path'])['text']\n",
    "    pred.append(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "633faf7a-8d6f-469f-a8e5-6ac6596b9145",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labels = fleur_trans_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4c696344-bc4d-4874-9f59-7203f8254c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from whisper_normalizer.basic import BasicTextNormalizer\n",
    "\n",
    "normalizer = BasicTextNormalizer()\n",
    "\n",
    "for i in range(len(pred)):\n",
    "    pred[i] = normalizer(pred[i])\n",
    "    labels[i] = normalizer(labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4df70c10-520b-415d-9cf8-4a88b4fee88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu = evaluate.load(\"wer\")\n",
    "results = bleu.compute(predictions=pred, references=labels)\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
